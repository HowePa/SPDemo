#!/usr/bin/env python

from confluent_kafka import Producer
import socket
from multiprocessing import Pool
import os
import argparse
from time import time, sleep
from datetime import datetime
import random
from faker import Faker
import logging


logging.basicConfig(level = logging.INFO,format = '%(asctime)s - %(process)d - [%(levelname)s]: %(message)s', datefmt='%Y/%m/%d %I:%M:%S')
logger = logging.getLogger(__name__)


def data_gen(max_size):
    faker = Faker(locale='zh_CN')
    while True:
        nitems = random.randint(max(1, max_size / 10), max_size)
        values = ""
        for _ in range(nitems):
            values += f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}|{random.randint(1, 100)}|{faker.name()}|{faker.address()}|{faker.email()}|{faker.msisdn()}\n'
        yield nitems, values


def parse_args():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        description="Multi Producer",
    )
    parser.add_argument(
        "--host",
        default=os.getenv("SP_KAFKA_HOST"),
        help="host of kafka."
    )
    parser.add_argument(
        "--port",
        default=os.getenv("SP_KAFKA_PORT"),
        help="port of kafka."
    )
    parser.add_argument(
        "--topic",
        default=os.getenv("SP_KAFKA_TOPIC"),
        help="topic of kafka."
    )
    parser.add_argument(
        "--process",
        type=int,
        default=3,
        choices=range(1, max(2, os.cpu_count() - 3)),
        help="size of process pool(also num of processes)."
    )
    parser.add_argument(
        "--time",
        type=int,
        default=2,
        help="running time(hour) for echo process."
    )
    parser.add_argument(
        "--interval",
        type=int,
        default=5,
        help="max interval(second) between production of each data set."
    )
    parser.add_argument(
        "--size",
        type=int,
        default=1000,
        help="max number of items per data set."
    )
    args = parser.parse_args()
    
    return args


def single_producer(args):
    conf={
        'bootstrap.servers': f'{args.host}:{args.port}',
        'client.id': socket.gethostname()
    }
    dataset = data_gen(args.size)
    start_time = time()
    nitems = 0
    logn = int(nitems / (args.size * 100))
    try:
        producer = Producer(conf)
        while True:
            data = next(dataset)
            producer.produce(args.topic, value=data[1])
            producer.flush()
            nitems += data[0]
            if int(nitems / (args.size * 100)) > logn:
                logger.info(f"produced {nitems} messages.")
                logn = int(nitems / (args.size * 100))

            sleep(random.randint(0, args.interval))
            end_time = time() - start_time
            if int(end_time / 3600) >= args.time:
                logger.info(f"finished after {int(end_time / 3600)} hours produced {nitems} items.")
                break
    except Exception as e:
        logger.error(str(e))


if __name__ == "__main__":
    args = parse_args()
    
    if not args.host or not args.port or not args.topic:
        logger.error("lost kafka args.")
        exit(1)
    
    logger.info(f"init producer with {args}")
    pool = Pool(processes=args.process)
    pool.map(single_producer, [args for _ in range(args.process)])
