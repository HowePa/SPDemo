#!/usr/bin/env python

from confluent_kafka import Producer
import socket
from multiprocessing import Pool
import os
import argparse
from time import time, sleep
from datetime import datetime
import random
from faker import Faker
import logging


logging.basicConfig(level = logging.INFO,format = '%(asctime)s - %(process)d - [%(levelname)s]: %(message)s', datefmt='%Y/%m/%d %I:%M:%S')
logger = logging.getLogger(__name__)

TOPIC=os.getenv("SP_KAFKA_TOPIC")
CONF={
    'bootstrap.servers': f'{os.getenv("SP_KAFKA_HOST")}:{os.getenv("SP_KAFKA_PORT")}',
    'client.id': socket.gethostname()
}
faker = Faker(locale='zh_CN')


def parse_args():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        description="Multi Producer",
    )
    # parser.add_argument(
    #     "--host",
    #     required=True,
    #     help="host of kafka."
    # )
    # parser.add_argument(
    #     "--port",
    #     required=True,
    #     help="port of kafka."
    # )
    # parser.add_argument(
    #     "--topic",
    #     required=True,
    #     help="topic of kafka."
    # )
    parser.add_argument(
        "--process",
        type=int,
        default=3,
        help="size of process pool(also num of processes)."
    )
    parser.add_argument(
        "--time",
        type=int,
        default=1,
        help="running time(h) for echo process."
    )
    args = parser.parse_args()
    
    return args


def single_producer(conf):
    start_time = time()
    count = 0
    producer = Producer(CONF)
    try:
        while True:
            value = ""
            for _ in range(random.randint(1, 20)):
                value += f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}|{random.randint(1, 100)}|{faker.address()}\n'
            producer.produce(TOPIC, value=value)
            count += 1
            if count % 10000 == 0:
                logger.info(f"produced {count} messages.")
            producer.flush()
            end_time = time() - start_time
            if int(end_time / 3600) > conf[1]:
                logger.info(f"finish after {int(end_time / 3600)} hours.")
                break
    except Exception as e:
        logger.error(str(e))


if __name__ == "__main__":
    args = parse_args()
    pool = Pool(processes=args.process)
    logger.info(f"init {args.process} producers.")
    pool.map(single_producer, [args.time for _ in range(args.process)])
